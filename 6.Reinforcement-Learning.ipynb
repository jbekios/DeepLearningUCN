{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1-5X9OUkA-C2Ih1gOS9Jd7GmkTWUEpDg1)\n",
    "\n",
    "# Laboratorio 06: Aprendizaje por reforzamiento\n",
    "\n",
    "## _Deep Learning_\n",
    "   \n",
    "<center>\n",
    "    <img src='images/diagram-rl.png'style=\"width: 600px;\">\n",
    "</center>\n",
    "\n",
    "**Profesor**: Juan Bekios Calfa\n",
    "\n",
    "**Carreras**: ICCI, IECI e IenCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "* Considere el escenario de enseñarle nuevos trucos a un perro. El perro no entiende nuestro idioma, por lo que no podemos decirle qué hacer. En cambio, seguimos una estrategia diferente. \n",
    "* Emulamos una situación (o una señal) y el perro intenta responder de muchas formas diferentes.\n",
    "  * Si la respuesta del perro es la deseada, lo premiamos con bocadillos. \n",
    "  * Del mismo modo, los perros tenderán a aprender qué no hacer cuando se enfrenten a experiencias negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "Elementos identificados:\n",
    "* El perro es un **\"agente\"** que está expuesto al **medio ambiente**. El **medio ambiente** podría ser una casa, contigo.\n",
    "* Las situaciones a los que el agente se encuentra son análogas a un **estado**. Un ejemplo de un **estado** podría ser un perro parado.\n",
    "* Los **agentes** reaccionan realizando una **acción** para pasar de un \"estado\" a otro \"estado\". Por ejemplo, su perro pasa de estar de pie a sentado.\n",
    "* Después de la transición, pueden recibir una recompensa o una penalización a cambio. ¡Les das un regalo! O un \"No\" como penalización.\n",
    "* La **política** es la estrategia de elegir una **acción** desde un estado con la expectativa de obtener mejores resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "El **aprendizaje por refuerzo** se encuentra entre el espectro del **aprendizaje supervisado** y el **aprendizaje no supervisado**. Sin embargo, hay algunos elementos importantes a considerar:\n",
    "\n",
    "1. **Ser codicioso no siempre funciona**. Hay cosas que son fáciles de hacer para obtener una gratificación instantánea, y hay cosas que proporcionan recompensas a largo plazo. El objetivo es **no ser codicioso** buscando recompensas inmediatas rápidas, sino optimizar para obtener las máximas recompensas todo el entrenamiento.\n",
    "2. **La secuencia importa en el aprendizaje por refuerzo**. El agente **recompenzado** no solo depende del estado actual, sino de toda la historia de los estados. A diferencia del aprendizaje supervisado y no supervisado, el tiempo es importante aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El proceso de aprendizaje por refuerzo\n",
    "\n",
    "<center>\n",
    "    <img src='images/rl-animation.gif'style=\"width: 600px;\">\n",
    "</center>\n",
    "\n",
    "El **aprendizaje por refuerzo** es la ciencia de tomar decisiones óptimas utilizando experiencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proceso\n",
    "\n",
    "\n",
    "1. Observación del medio ambiente.\n",
    "2. Decidir cómo actuar usando alguna estrategia.\n",
    "3. Actuando en consecuencia.\n",
    "4. Recibir una recompensa o una penalización.\n",
    "5. Aprender de las experiencias y perfeccionar nuestra estrategia.\n",
    "6. Iterar hasta encontrar una estrategia óptima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problema:\n",
    "\n",
    "La tarea de un _Smartcap_ es recoger a un pasajero de un lugar y dejarlo en otro. Aquí hay algunas especificaciones solicitadas para resolver el problema:\n",
    "\n",
    "* Deje al pasajero en el lugar correcto.\n",
    "* Ahorre tiempo a pasajero tomando el mínimo tiempo.\n",
    "* Cuidar las normas de seguridad y tráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelamiento para el aprendizaje automático\n",
    "\n",
    "### 1. Premios\n",
    "\n",
    "* El agente debe recibir una alta **recompensa positiva** por un abandono exitoso.\n",
    "* El agente debe ser **penalizado** si intenta dejar a un pasajero en lugares incorrectos.\n",
    "* El agente debería obtener una recompensa levemente negativa por no llegar al destino después de cada paso de tiempo. La idea es que el agente llegue lo más rápido posible a la meta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelamiento para el aprendizaje automático\n",
    "\n",
    "### 2. Espacios de estado (I)\n",
    "\n",
    "En el aprendizaje por refuerzo, el agente se encuentra con un estado y luego actúa de acuerdo con el estado en el que se encuentra.\n",
    "\n",
    "El **espacio de estados** (_state space_) es el conjunto de todas las situaciones posibles en las que podría vivir nuestro taxi. El estado debe contener información útil que el agente necesita para realizar la acción correcta.\n",
    "\n",
    "El **espacio de estados** es el área de entrenamiento del _Smartcab_ donde le **enseñamos** a transportar personas de un **estacionamiento** a cuatro ubicaciones diferentes **(R, G, Y, B)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelamiento para el aprendizaje automático\n",
    "\n",
    "### 2. Espacios de estado (II)\n",
    "\n",
    "<center>\n",
    "    <img src='images/taxi.png'style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelamiento para el aprendizaje automático\n",
    "\n",
    "### 2. Espacio de estados (III)\n",
    "\n",
    "* _Smartcab_ es el único vehículo en el estacionamiento.\n",
    "* El estacionamiento se puede representar como una cuadrícula de $5\\times 5$.\n",
    "* El taxi puede estar en 25 ubicaciones. Por ejemplo, (3, 1) en la figura.\n",
    "* Hay cuatro ubicaciones donde podemos recoger al pasajero (R, G, Y, B).\n",
    "* También, se debe contabilizar **un estado adicional** dentro del taxi.\n",
    "* Hay 4 ubicaciones para destino.\n",
    "* 5 para posibles ubicaciones de toma de pasajeros.\n",
    "\n",
    "El **total de estados es**: $5\\times5\\times5\\times4=500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelamiento para el aprendizaje automático\n",
    "\n",
    "### 3. Espacios de acción\n",
    "\n",
    "El agente se encuentra en uno de los **500 estados** y realiza **una acción**. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger / dejar a un pasajero.\n",
    "\n",
    "Acciones posibles:\n",
    "\n",
    "1.    south\n",
    "2.    north\n",
    "3.    east\n",
    "4.    west\n",
    "5.    pickup (Tomar un pasajero)\n",
    "6.    dropoff ( Dejar un pasajero)\n",
    "\n",
    "El **taxi** no puede realizar ciertas por restricciones del **medio ambiente**. En el código, simplemente, se penaliza como -1 por cada golpe en la pared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementación con Python\n",
    "\n",
    "Para resolver este problema utilizaremos **Gym** (https://gym.openai.com/) que nos permite simular diferentes ambientas para realizar simulaciones.\n",
    "\n",
    "**Gym** proporciona diferentes **entornos de juego** que podemos conectar a nuestro código y probar un agente. La biblioteca se encarga de la API para proporcionar toda la información que nuestro agente requeriría, como posibles acciones, puntaje y estado actual. \n",
    "\n",
    "Usaremos el entorno Gym llamado `Taxi-V2`, del que se extrajeron todos los detalles explicados anteriormente. Los objetivos, recompensas y acciones son todos iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cargando ambiente para _Smartcab_ (Taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3').env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Librerías\n",
    "\n",
    "La interfaz principal del **Gym** es `env`, que es la interfaz del entorno unificado. Los siguientes son los envmétodos que nos serían de gran ayuda:\n",
    "\n",
    "`env.reset`: Restablece el entorno y devuelve un estado inicial aleatorio.\n",
    "`env.step(action)`: Paso el entorno en un paso de tiempo. Devoluciones\n",
    "* **observación** (_observation_) : Observaciones del medio ambiente\n",
    "* **recompensa** (_reward_): si su acción fue beneficiosa o no\n",
    "* **done** : Indica si hemos recogido y dejado a un pasajero, también llamado episodio\n",
    "* **info** : información adicional como el rendimiento y la latencia para fines de depuración\n",
    "    \n",
    "`env.render`: Renderiza un fotograma del entorno (útil para visualizar el entorno)\n",
    "\n",
    "Nota: Estamos usando el `.env` al final de `make` para evitar que el entrenamiento se detenga en 200 iteraciones, que es el valor predeterminado para la nueva versión de Gym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recordemos el problema\n",
    "\n",
    "\n",
    "Aquí está nuestra declaración de problema reestructurada (de los documentos de Gym):\n",
    "\n",
    "\"Hay 4 ubicaciones (etiquetadas con letras diferentes) y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada vez. paso que da. También hay una penalización de 10 puntos por acciones ilegales de recoger y dejar \".\n",
    "\n",
    "Sumérjase más en el medio ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Espacio de acción Discrete(6)\n",
      "Espacio de estado Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Espacio de acción {}\".format(env.action_space))\n",
    "print(\"Espacio de estado {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* El cuadrado amarillo representa al taxi.\n",
    "* \"|\" representa una pared.\n",
    "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La **letra azul** representa la **ubicación actual** y la **letra morada** es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los **espacio de acción** se codifican como: \n",
    "\n",
    "    0 = sur\n",
    "    1 = norte\n",
    "    2 = este\n",
    "    3 = oeste\n",
    "    4 = recogida\n",
    "    5 = abandono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Configurar el problema  de la figura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (fila del taxi , columna taxi,  indice pasajero, indice )\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "También, podemos establecer el estado del **entorno** manualmente `env.s` usando ese número codificado. Puede jugar con los números y verá que el taxi, el pasajero y el destino se mueven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tabla de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Estructura de diccionario**: {acción: [(probabilidad, próximo estado, recompensa, hecho-logrado)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunas cosas a tener en cuenta:\n",
    "\n",
    "*    El 0-5 corresponde a las acciones (sur, norte, este, oeste, recogida, bajada) que el taxi puede realizar en nuestro estado actual en la ilustración.\n",
    "*    En este entorno, la **probabilidad** siempre es 1.0.\n",
    "*    El **próximo estado** el estado en el que estaríamos si tomamos la acción en este índice del dict\n",
    "*    Todas las acciones de movimiento tienen una recompensa de -1 y las acciones de recoger/dejar tienen una recompensa de -10 en este **estado en particular**. Si estamos en un estado en el que el taxi tiene un pasajero y está en la parte superior del destino correcto, veríamos una recompensa de 20 en la acción de devolución (5).\n",
    "*    **Hecho-logrado** se utiliza para indicarnos cuándo hemos dejado a un pasajero en el lugar correcto. Cada abandono exitoso es el final de un episodio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resolviendo el ambiente sin aprendizaje por reforzamiento\n",
    "\n",
    "Resolveremos el problema por **fuerza bruta**.\n",
    "\n",
    "Dado que tenemos P (tabla de recompensas) predeterminadas en cada estado, podemos intentar que nuestro taxi navegue solo con eso.\n",
    "\n",
    "Crearemos un bucle infinito que se ejecutará hasta que un pasajero llegue a un destino (un episodio ), o en otras palabras, cuando la recompensa recibida sea 20. El `env.action_space.sample()` método selecciona automáticamente una acción aleatoria del conjunto de todas las acciones posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 209\n",
      "Penalties incurred: 63\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 209\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.2)\n",
    "        \n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
